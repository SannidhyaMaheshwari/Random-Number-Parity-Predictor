# -*- coding: utf-8 -*-
"""randomNumberpredict-exp1ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LLmgh82shq24ZIBeZ69F6a7u9o9H7n9F
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from tensorflow.keras.models import Sequential
from keras.layers import LSTM, Bidirectional, Dense, Embedding ,GRU
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import os

# Load dataset (assuming a CSV file with a column 'number')
def load_data(file_path):
    df = pd.read_csv(file_path, header=None, names=['number'])
    df['parity'] = df['number'] % 2  # 0 for even, 1 for odd
    return df

feature_file = 'processed_features.csv'
file_path = 'random_numbers.csv'
history_size = 32

# Check if processed feature file exists
if os.path.exists(feature_file):
    print("Loading precomputed features...")
    feature_data = pd.read_csv(feature_file, header=None)
    X = feature_data.iloc[:, :-1].values
    y = feature_data.iloc[:, -1].values
else:
    print("Processing features from scratch...")
    data = load_data(file_path)

    X, y = [], []
    for i in range(history_size, len(data) - 1):  # Ensure next value is available
        # Features: Last 'history_size' numbers + last 'history_size' parity values
        features = np.concatenate((
            data['number'].iloc[i-history_size:i].values,  # Last N numbers
            data['parity'].iloc[i-history_size:i].values   # Last N parity values
        ))
        X.append(features)

        # Target: Parity of the next number
        y.append(data['parity'].iloc[i + 1])

    X, y = np.array(X), np.array(y)

    # Save processed features
    feature_df = pd.DataFrame(np.column_stack((X, y)))
    feature_df.to_csv(feature_file, index=False, header=False)

# Train-test split (50% training, 50% testing)
train_size = int(len(X) * 0.5)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Store results
results = {}

print(X[:5])



models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=500)
}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f'{name} Accuracy: {accuracy:.4f}')

# LSTM Model (without Embedding)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

lstm_model = Sequential([
    LSTM(32, return_sequences=False, input_shape=(X_train.shape[1], 1)),
    Dense(1, activation='sigmoid')
])
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f'LSTM Accuracy: {lstm_accuracy:.4f}')

# BiLSTM Model (without Embedding)
bilstm_model = Sequential([
    Bidirectional(LSTM(32, return_sequences=False, input_shape=(X_train.shape[1], 1))),
    Dense(1, activation='sigmoid')
])
bilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=2)[1]
print(f'BiLSTM Accuracy: {bilstm_accuracy:.4f}')

# GRU Model (without Embedding)
gru_model = Sequential([
    GRU(32, return_sequences=False, input_shape=(X_train.shape[1], 1)),
    Dense(1, activation='sigmoid')
])
gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=2)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=2)[1]
print(f'GRU Accuracy: {gru_accuracy:.4f}')

import matplotlib.pyplot as plt
import seaborn as sns

# Accuracy results in percentage (rounded to 2 decimal places)
results = {
    'Logistic Regression': round(0.5000 * 100, 2),
    'Random Forest': round(0.5496 * 100, 2),
    'Neural Network': round(0.5002 * 100, 2),
    'LSTM': round(0.4999 * 100, 2),
    'BiLSTM': round(0.5001 * 100, 2),
    'GRU': round(0.4999 * 100, 2)
}

model_names = list(results.keys())
accuracies = list(results.values())

# Set style
sns.set_style("whitegrid")

# Create Bar Graph
plt.figure(figsize=(10, 5))
sns.barplot(x=model_names, y=accuracies, palette="coolwarm")
plt.title("Model Accuracy Comparison (Bar Chart)", fontsize=14, fontweight="bold")
plt.ylabel("Accuracy (%)", fontsize=12)
plt.ylim(49, 56)
plt.xticks(rotation=45)

# Add text labels on bars
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.1, f"{v}%", ha='center', fontsize=10, fontweight='bold')

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")

# Create Line Plot
plt.figure(figsize=(10, 5))
sns.lineplot(x=model_names, y=accuracies, marker="o", color="b", linewidth=2)

plt.title("Model Accuracy Trend (Line Graph)", fontsize=14, fontweight="bold")
plt.ylabel("Accuracy (%)", fontsize=12)
plt.ylim(49, 56)
plt.xticks(rotation=45)

# Add text labels
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.1, f"{v}%", ha='center', fontsize=10, fontweight='bold')

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")

# Create Scatter Plot
plt.figure(figsize=(10, 5))
sns.scatterplot(x=model_names, y=accuracies, color="red", s=100)

plt.title("Model Accuracy Distribution (Scatter Plot)", fontsize=14, fontweight="bold")
plt.ylabel("Accuracy (%)", fontsize=12)
plt.ylim(49, 56)
plt.xticks(rotation=45)

# Add text labels
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.1, f"{v}%", ha='center', fontsize=10, fontweight='bold')

plt.show()